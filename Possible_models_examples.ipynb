{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce5ddc6",
   "metadata": {},
   "source": [
    "<font color='orange'>SKLEARN</font> - https://scikit-learn.org/stable/modules/semi_supervised.html\n",
    "\n",
    "LAMDA-SSL - https://ygzwqzd.github.io/LAMDA-SSL/#/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fc098c",
   "metadata": {},
   "source": [
    "# <font color='blue'>Wrapper methods</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6014b28",
   "metadata": {},
   "source": [
    "## <font color='green'>Self-training</font><font color='orange'> SKLEARN</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10aebb",
   "metadata": {},
   "source": [
    "## <font color='green'>Tri-training</font>, LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Inductive -> Wrapper methods -> Co-training -> Single-view co-training -> <font color='green'>Tri-training</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[A survey on semi-supervised learning]\n",
    "\n",
    "Trzy klasyfikatory są naprzemiennie trenowane. Główna idea polega na tym, że gdy dwóch z trzech klasyfikatorów zgadza się co do prognozy dla danego punktu danych, przyjmuje się tę prognozę. Co ważne, \"tri-training\" <b>nie polega</b> na probabilistycznych prognozach indywidualnych klasyfikatorów. [A survey on semi-supervised learning]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40dec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.Tri_Training import Tri_Training\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/Tri_Training_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "SVM=SVC(C=1.0,kernel='linear',probability=True,gamma='auto')\n",
    "\n",
    "model=Tri_Training(base_estimator=SVM,evaluation=evaluation,file=file,verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "print(result,file=file)\n",
    "\n",
    "print(performance,file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a4db95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9766081871345029,\n",
       " 'precision': 0.9819819819819819,\n",
       " 'Recall': 0.96875,\n",
       " 'F1': 0.97469665581533,\n",
       " 'AUC': 0.9970794392523364,\n",
       " 'Confusion_matrix': array([[0.9375, 0.0625],\n",
       "        [0.    , 1.    ]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87c39cc",
   "metadata": {},
   "source": [
    "### Własna implementacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "\n",
    "class TriTraining:\n",
    "    def __init__(self, classifier1, classifier2, classifier3):\n",
    "        self.clf1 = classifier1\n",
    "        self.clf2 = classifier2\n",
    "        self.clf3 = classifier3\n",
    "\n",
    "    def fit(self, labeled_data, labeled_labels, unlabeled_data):\n",
    "        # Początkowe dopasowanie klasyfikatorów do etykietowanych danych\n",
    "        self.clf1.fit(labeled_data, labeled_labels)\n",
    "        self.clf2.fit(labeled_data, labeled_labels)\n",
    "        self.clf3.fit(labeled_data, labeled_labels)\n",
    "\n",
    "        # Pętla uczenia klasyfikatorów na podstawie danych pseudoetykietowanych\n",
    "        while unlabeled_data.shape[0] > 0:\n",
    "            # Prognozy klasyfikatorów dla nieoznaczonych danych\n",
    "            pred1 = self.clf1.predict(unlabeled_data)\n",
    "            pred2 = self.clf2.predict(unlabeled_data)\n",
    "            pred3 = self.clf3.predict(unlabeled_data)\n",
    "\n",
    "            # Znajdź indeksy, gdzie dwóch klasyfikatorów zgadza się co do prognozy\n",
    "            idx12, = np.where(pred1 == pred2)\n",
    "            idx13, = np.where(pred1 == pred3)\n",
    "            idx23, = np.where(pred2 == pred3)\n",
    "\n",
    "            # Ucz klasyfikatory na danych pseudoetykietowanych\n",
    "            if len(idx12) > 0:\n",
    "                self.clf3.fit(unlabeled_data[idx12], pred1[idx12])\n",
    "            if len(idx13) > 0:\n",
    "                self.clf2.fit(unlabeled_data[idx13], pred1[idx13])\n",
    "            if len(idx23) > 0:\n",
    "                self.clf1.fit(unlabeled_data[idx23], pred2[idx23])\n",
    "\n",
    "            # Usuń użyte dane z nieoznaczonych danych\n",
    "            unlabeled_data = np.delete(unlabeled_data, np.concatenate([idx12, idx13, idx23]), axis=0)\n",
    "\n",
    "    def predict(self, data):\n",
    "        pred1 = self.clf1.predict(data)\n",
    "        pred2 = self.clf2.predict(data)\n",
    "        pred3 = self.clf3.predict(data)\n",
    "\n",
    "        # Głosowanie większościowe dla prognozy\n",
    "        final_pred = []\n",
    "        for p1, p2, p3 in zip(pred1, pred2, pred3):\n",
    "            if p1 == p2 or p1 == p3:\n",
    "                final_pred.append(p1)\n",
    "            else:\n",
    "                final_pred.append(p2)\n",
    "        return np.array(final_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb7e2c",
   "metadata": {},
   "source": [
    "##  <font color='green'>ASSEMBLE</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Inductive -> Wrapper methods -> Boosting -> <font color='green'>ASSEMBLE</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[A survey on semi-supervised learning]\n",
    "\n",
    "Pseudo-etykietowanie: W algorytmie \"ASSEMBLE\", dane, które początkowo nie mają etykiety, są etykietowane (pseudo-etykietowane) przez model w każdej iteracji. To pozwala na wykorzystanie tych punktów danych do trenowania klasyfikatora w kolejnych iteracjach.\n",
    "\n",
    "- Adaptacyjna konstrukcja klasyfikatora: Te pseudo-etykietowane punkty danych są następnie wykorzystywane do budowy lub dostosowywania klasyfikatora. Dzięki temu klasyfikator może stopniowo uczyć się z coraz większej ilości danych, co może prowadzić do lepszej generalizacji.\n",
    "\n",
    "- Maksymalizacja marginesu klasyfikacji: Jednym z głównych celów \"ASSEMBLE\" jest maksymalizacja marginesu klasyfikacji w przestrzeni funkcji. Margines klasyfikacji to odległość między punktami danych a granicą decyzyjną klasyfikatora. Maksymalizując ten margines, algorytm dąży do lepszego rozdzielania klas.\n",
    "\n",
    "- Wykorzystanie bootstrapingu: Problemem w pseudo-etykietowaniu jest to, że nie jest pewne, które punkty danych powinny być wykorzystywane w następnej iteracji uczenia. Aby rozwiązać ten problem, \"ASSEMBLE\" korzysta z techniki bootstrapingu. Oznacza to, że próbkuje się równomiernie i losowo z etykietowanych i pseudo-etykietowanych danych, pozwalając modelowi uczyć się z różnorodnego zestawu danych.\n",
    "\n",
    "- Pomysł stojący za \"ASSEMBLE\" i innymi technikami uczenia półnadzorowanego polega na wykorzystaniu niewykorzystanych informacji zawartych w nieoznaczonych danych. Pseudo-etykietowanie, choć niosące pewne ryzyko wprowadzenia błędnych etykiet, może znacznie zwiększyć ilość dostępnych danych do uczenia, co jest szczególnie przydatne w sytuacjach, gdy oznaczone dane są trudne lub kosztowne do uzyskania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb468c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9766081871345029,\n",
       " 'precision': 0.9819819819819819,\n",
       " 'Recall': 0.96875,\n",
       " 'F1': 0.97469665581533,\n",
       " 'AUC': 0.9978095794392523,\n",
       " 'Confusion_matrix': array([[0.9375, 0.0625],\n",
       "        [0.    , 1.    ]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.Assemble import Assemble\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/Assemble_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "# Base estimater\n",
    "SVM=SVC(probability=True)\n",
    "\n",
    "model=Assemble(T=100,base_estimator=SVM,evaluation=evaluation,verbose=True,file=file)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83335c3d",
   "metadata": {},
   "source": [
    "### Własna implementacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914b8f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import resample\n",
    "\n",
    "class ASSEMBLE:\n",
    "\"\"\"\n",
    "    Implementacja metody ASSEMBLE do uczenia pół-nadzorowanego.\n",
    "\n",
    "    Podstawy ASSEMBLE:\n",
    "    -----------------\n",
    "    Metoda `ASSEMBLE` jest strategią uczenia pół-nadzorowanego, w której klasyfikator bazowy jest szkolony wielokrotnie,\n",
    "    przy czym w każdej iteracji dane nieoznaczone są etykietowane (pseudo-labeling) i łączone z rzeczywistymi danymi etykietowanymi.\n",
    "    Główna idea polega na tym, żeby wykorzystać moc klasyfikatora bazowego do \"pseudo-etykietowania\" danych nieoznaczonych i \n",
    "    następnie trenować klasyfikator na kombinacji danych oznaczonych i pseudo-etykietowanych.\n",
    "\n",
    "    Metoda `fit`:\n",
    "    ------------\n",
    "    1. Łączenie danych: Dane oznaczone i nieoznaczone są łączone w jeden zestaw danych (`X_combined`).\n",
    "    2. Pętla przez iteracje: W każdej iteracji model jest trenowany na danych oznaczonych, a następnie jest używany do przewidywania\n",
    "       etykiet dla danych nieoznaczonych (`pseudo_labels`).\n",
    "    3. Bootstrap: Następnie tworzony jest zestaw danych bootstrap poprzez losowe próbkowanie (z powtórzeniami) z połączonych \n",
    "       danych oznaczonych i pseudo-etykietowanych.\n",
    "    4. Podział na oznaczone i nieoznaczone: Po stworzeniu zestawu bootstrap, dzielimy go z powrotem na dane oznaczone i \n",
    "       nieoznaczone na potrzeby następnej iteracji.\n",
    "\n",
    "    Metoda `predict`:\n",
    "    ----------------\n",
    "    Po zakończeniu procesu uczenia się mamy wiele nauczonych modeli. W metodzie `predict` zbieramy przewidywania z każdego modelu \n",
    "    dla danego zestawu danych X. Następnie, dla każdego punktu danych, decydujemy o końcowej etykiecie na podstawie głosowania \n",
    "    większościowego spośród wszystkich modeli.\n",
    "\"\"\"\n",
    "    def __init__(self, base_classifier, n_iterations=10):\n",
    "        self.base_classifier = base_classifier\n",
    "        self.n_iterations = n_iterations\n",
    "        self.models = []\n",
    "    \n",
    "    def fit(self, X_labeled, y_labeled, X_unlabeled):\n",
    "        X_combined = np.vstack((X_labeled, X_unlabeled))\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            model = clone(self.base_classifier)\n",
    "            model.fit(X_labeled, y_labeled)\n",
    "            self.models.append(model)\n",
    "            \n",
    "            pseudo_labels = model.predict(X_unlabeled)\n",
    "            \n",
    "            X_bootstrap, y_bootstrap = resample(X_combined, np.hstack((y_labeled, pseudo_labels)))\n",
    "            \n",
    "            X_labeled, X_unlabeled = X_bootstrap[:len(X_labeled)], X_bootstrap[len(X_labeled):]\n",
    "            y_labeled = y_bootstrap[:len(y_labeled)]\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n",
    "\n",
    "# Example usage:\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# data = load_iris()\n",
    "# X_labeled, y_labeled = data.data[:30], data.target[:30]\n",
    "# X_unlabeled = data.data[30:]\n",
    "\n",
    "# model = ASSEMBLE(DecisionTreeClassifier(), n_iterations=5)\n",
    "# model.fit(X_labeled, y_labeled, X_unlabeled)\n",
    "# predictions = model.predict(data.data[30:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51ab8d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier: 0.97\n",
      "Accuracy of ASSEMBLE DecisionTreeClassifier: 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Wczytanie danych\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.6, random_state=42)\n",
    "\n",
    "# Dzielenie zbioru treningowego na dane oznaczone i nieoznaczone\n",
    "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X_train, y_train, test_size=0.6, random_state=42)\n",
    "\n",
    "# Trening i testowanie zwykłego DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_labeled, y_labeled)\n",
    "predictions_tree = clf.predict(X_test)\n",
    "accuracy_tree = accuracy_score(y_test, predictions_tree)\n",
    "print(f\"Accuracy of DecisionTreeClassifier: {accuracy_tree:.2f}\")\n",
    "\n",
    "# Trening i testowanie ASSEMBLE DecisionTreeClassifier\n",
    "assemble_model = ASSEMBLE(DecisionTreeClassifier(), n_iterations=5)\n",
    "assemble_model.fit(X_labeled, y_labeled, X_unlabeled)\n",
    "predictions_assemble = assemble_model.predict(X_test)\n",
    "accuracy_assemble = accuracy_score(y_test, predictions_assemble)\n",
    "print(f\"Accuracy of ASSEMBLE DecisionTreeClassifier: {accuracy_assemble:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada321f4",
   "metadata": {},
   "source": [
    "##  <font color='green'>Semiboost</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Inductive -> Wrapper methods -> Boosting -> <font color='green'>Semiboost</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[A survey on semi-supervised learning]\n",
    "\n",
    "Algorytm uczenia pół-nadzorowanego o nazwie SemiBoost rozwiązuje problem wyboru danych używanych przez klasyfikatory bazowe, opierając się na założeniu rozmieszczenia punktów na pewnej rozmaitości (manifold assumption) i korzystając z metod opartych na grafach. Dla każdego punktu z nieoznakowanych danych przydzielana jest pseudo-etykieta, a następnie obliczane jest zaufanie do tego przypisania. Na podstawie grafu, który koduje podobieństwo między danymi, pewien podzbiór tych pseudo-etykietowanych danych jest dodawany do zbioru etykietowanych danych dla kolejnego klasyfikatora. SemiBoost był skutecznie stosowany, między innymi, do śledzenia obiektów na wideo.\n",
    "\n",
    "SemiBoost wykorzystuje standardowy model klasyfikacji z boostingu, gdzie końcowa etykieta jest liniową kombinacją przewidywań poszczególnych uczących się. W kontekście SemiBoost, sukces znaczenia testowego powinien spełniać trzy wymagania:\n",
    "\n",
    "- Przewidywane etykiety dla nieoznakowanych danych powinny być spójne dla punktów blisko siebie.\n",
    "- Przewidywane etykiety dla nieoznakowanych danych powinny być spójne z etykietami pobliskich oznakowanych danych.\n",
    "- Przewidywane etykiety dla oznakowanych danych powinny odpowiadać ich prawdziwym etykietom.\n",
    "\n",
    "Te wymagania są przedstawione w postaci problemu optymalizacji z ograniczeniami. SemiBoost używa boostingu do rozwiązania tego problemu optymalizacji, gdzie dwie pierwsze cechy są uchwycone przez funkcję celu, a ostatnia jest narzucana jako ograniczenie. Wzór matematyczny przedstawia koszty wynikające z niespójności między nieoznakowanymi i łączonymi etykietowanymi oraz nieoznakowanymi danymi.\n",
    "\n",
    "\n",
    "----------------------------\n",
    "Streszczenie\n",
    "\n",
    "SemiBoost to algorytm semi-nadzorowanego uczenia się, który łączy techniki boostingu z metodami opartymi na grafach. Oto kilka kluczowych koncepcji dotyczących SemiBoost:\n",
    "\n",
    "1. Manifold Assumption: SemiBoost opiera się na tzw. założeniu manifoldowym. To założenie mówi, że podobne dane wejściowe mają tendencję do otrzymywania podobnych etykiet. Innymi słowy, jeśli dwa punkty danych są blisko siebie w przestrzeni, prawdopodobnie należą do tej samej klasy. W kontekście SemiBoost i wielu innych algorytmów semi-nadzorowanego uczenia się, kernel RBF jest używany do mierzenia podobieństwa (lub odległości) między punktami danych.\n",
    "\n",
    "2. Pseudo-etykietowanie: Każdy nieoznakowany punkt danych otrzymuje pseudo-etykietę, a następnie obliczane jest zaufanie do tego przydziału. Na podstawie tej pewności oraz struktury manifoldowej danej części danych, wybrane są punkty danych do dalszego trenowania.\n",
    "3. Boosting: Podobnie jak w standardowych algorytmach boostingu, SemiBoost tworzy liniową kombinację klasyfikatorów bazowych, z których każdy skupia się na trudnych do klasyfikacji przykładach z poprzednich iteracji.\n",
    "4. Optymalizacja z ograniczeniami: Kluczowym aspektem SemiBoost jest postawienie problemu jako optymalizacji z ograniczeniami. Algorytm stara się minimalizować niespójność etykiet między danymi etykietowanymi i nieoznakowanymi, jednocześnie zapewniając, że przewidywane etykiety dla danych etykietowanych odpowiadają ich prawdziwym etykietom.\n",
    "5. Metody oparte na grafach: SemiBoost korzysta z technik grafowych do określenia podobieństwa między punktami danych. Wykorzystuje to do zapewnienia, że etykiety są spójne dla punktów danych leżących na tym samym manifoldzie.\n",
    "\n",
    "Zastosowania: Choć algorytm SemiBoost można stosować w różnych dziedzinach, był on szczególnie skuteczny w zadaniach takich jak śledzenie obiektów w sekwencjach wideo, co pokazuje jego zdolność do wykorzystywania zarówno etykietowanych, jak i nieoznakowanych danych w trudnych warunkach.\n",
    "\n",
    "Podsumowując, SemiBoost to zaawansowany algorytm semi-nadzorowanego uczenia się, który łączy moc technik boostingu z zaawansowanymi technikami grafowymi w celu poprawy wydajności w zadaniach klasyfikacji.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c19acf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9473684210526315,\n",
       " 'precision': 0.9518008474576272,\n",
       " 'Recall': 0.9359667056074766,\n",
       " 'F1': 0.9428666889408619,\n",
       " 'AUC': 0.9929906542056075,\n",
       " 'Confusion_matrix': array([[0.890625  , 0.109375  ],\n",
       "        [0.01869159, 0.98130841]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.SemiBoost import SemiBoost\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/SemiBoost_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "SVM=SVC(C=1.0,kernel='rbf',probability=True,gamma='auto')\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "model=SemiBoost(gamma=10,base_estimator=SVM,evaluation=evaluation,file=file,verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346624c",
   "metadata": {},
   "source": [
    "# <font color='blue'>Unsupervised preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d8d79",
   "metadata": {},
   "source": [
    "brak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a8cd17",
   "metadata": {},
   "source": [
    "# <font color='blue'>Intrinsically semi-supervised</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ef759",
   "metadata": {},
   "source": [
    "## <font color='green'>LapSVM</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Manifolds -> Manifold regularization -> <font color='green'>LapSVM</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[Dokumentacja LAMDA-SSL]\n",
    "\n",
    "Bazuje ona na założeniu manifoldu. Klasyczne SVM oparte na oryginalnej przestrzeni cech pomija kluczowe cechy próbek. LapSVM dodaje termin regularyzacji manifoldu do celu optymalizacyjnego SVM, aby nauczyć się istotnego rozkładu próbek. LapSVM buduje model grafu wykorzystując wszystkie próbki, uzyskuje macierz wag modelu grafu przez podobieństwo cech próbek i oblicza jego macierz Laplace'a. Ten termin regularyzacji prowadzi do spójności przewidywanych wyników dla sąsiadujących próbek w grafie. W przeciwieństwie do TSVM, LapSVM kara tylko błędne klasyfikacje próbek oznaczonych, ale wykorzystuje wszystkie próbki podczas budowy modelu grafu, dzięki czemu nieoznaczone próbki uczestniczą w procesie uczenia poprzez rozkład próbek na rozmaitości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb99957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9649122807017544,\n",
       " 'precision': 0.9691441441441442,\n",
       " 'Recall': 0.9562646028037383,\n",
       " 'F1': 0.962044983722995,\n",
       " 'AUC': 0.9944509345794392,\n",
       " 'Confusion_matrix': array([[0.921875  , 0.078125  ],\n",
       "        [0.00934579, 0.99065421]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.LapSVM import LapSVM\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/LapSVM_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "model=LapSVM(neighbor_mode='connectivity',\n",
    "           gamma_d=0.03,\n",
    "           n_neighbor= 5,\n",
    "           gamma_k=0.03,\n",
    "           gamma_A= 0.03,\n",
    "           gamma_I= 0,\n",
    "           evaluation=evaluation,file=file,verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b20e5",
   "metadata": {},
   "source": [
    "## <font color='green'>Ladder networks</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Perturbation-based -> <font color='green'>Ladder networks</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[LAMDA-SSL]\n",
    "- Zaproponowany przez Rasmusa i innych.\n",
    "- Struktura bazująca na autoenkoderze, gdzie wyjścia z ostatniej warstwy enkodera są traktowane jako etykiety miękkie.\n",
    "- Sieć przyjmuje dwa metody kodowania: klasyczne kodowanie bez szumu oraz kodowanie z szumem, gdzie szum jest dodawany do wejść każdej warstwy.\n",
    "- Pierwszym krokiem jest kodowanie z szumem i bez szumu próbek. Następnie uzyskuje się reprezentacje zaszumione i niezaszumione każdej warstwy.\n",
    "- Dekoder jest używany do dekodowania wyniku kodowania z szumem, uzyskując zaszumione reprezentacje dekodowania dla każdej warstwy.\n",
    "- Wykorzystywana jest funkcja straty błędu średniokwadratowego (MSE) do obliczania niespójności między reprezentacją kodowaną bez szumu a reprezentacją dekodowaną z szumem na każdym poziomie. To obejmuje również oryginalne dane wejściowe jako warstwę zerową.\n",
    "- Wstępnie określone wagi są używane do określenia wag niespójności dla każdej warstwy.\n",
    "- Niespójności są ważone hierarchicznie w funkcji straty bez nadzoru, co zwiększa odporność modelu.\n",
    "- Regularyzacja spójności w Ladder Network używa zaszumionej reprezentacji kodowanej jako \"most\" do penalizacji niespójności między reprezentacją kodowaną bez szumu a zaszumioną reprezentacją dekodowaną.\n",
    "- Dzięki temu autoenkoder może uzyskać spójne reprezentacje enkodera i dekodera na wszystkich poziomach. Ponadto reprezentacje warstwy ukrytej pozostają spójne, niezależnie od tego, czy dodany jest szum.\n",
    "- Dzięki temu model jest odporny na zakłócenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581f6433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8563,\n",
       " 'top_5_accuracy': 0.9683,\n",
       " 'precision': 0.8668545707173398,\n",
       " 'Recall': 0.8545707723710525,\n",
       " 'F1': 0.8534983564223332,\n",
       " 'AUC': 0.9823251238971753,\n",
       " 'Confusion_matrix': array([[9.69387755e-01, 0.00000000e+00, 0.00000000e+00, 4.08163265e-03,\n",
       "         0.00000000e+00, 2.04081633e-03, 1.63265306e-02, 1.02040816e-03,\n",
       "         4.08163265e-03, 3.06122449e-03],\n",
       "        [0.00000000e+00, 9.87665198e-01, 0.00000000e+00, 4.40528634e-03,\n",
       "         0.00000000e+00, 8.81057269e-04, 5.28634361e-03, 0.00000000e+00,\n",
       "         1.76211454e-03, 0.00000000e+00],\n",
       "        [6.39534884e-02, 3.48837209e-02, 7.43217054e-01, 4.94186047e-02,\n",
       "         8.72093023e-03, 2.90697674e-03, 2.42248062e-02, 2.32558140e-02,\n",
       "         2.32558140e-02, 2.61627907e-02],\n",
       "        [2.57425743e-02, 6.93069307e-03, 3.96039604e-03, 8.48514851e-01,\n",
       "         0.00000000e+00, 5.14851485e-02, 5.94059406e-03, 1.98019802e-02,\n",
       "         1.68316832e-02, 2.07920792e-02],\n",
       "        [8.14663951e-03, 2.03665988e-02, 1.01832994e-03, 0.00000000e+00,\n",
       "         7.01629328e-01, 5.09164969e-03, 2.74949084e-02, 5.09164969e-03,\n",
       "         3.05498982e-03, 2.28105906e-01],\n",
       "        [4.82062780e-02, 5.60538117e-03, 0.00000000e+00, 3.02690583e-02,\n",
       "         3.36322870e-03, 8.44170404e-01, 2.91479821e-02, 2.01793722e-02,\n",
       "         4.48430493e-03, 1.45739910e-02],\n",
       "        [2.29645094e-02, 5.21920668e-03, 1.04384134e-03, 0.00000000e+00,\n",
       "         6.26304802e-03, 1.56576200e-02, 9.48851775e-01, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00],\n",
       "        [3.89105058e-03, 4.57198444e-02, 1.26459144e-02, 2.91828794e-03,\n",
       "         3.89105058e-03, 2.91828794e-03, 2.91828794e-03, 8.92996109e-01,\n",
       "         9.72762646e-04, 3.11284047e-02],\n",
       "        [6.36550308e-02, 2.87474333e-02, 7.18685832e-03, 4.82546201e-02,\n",
       "         2.05338809e-03, 7.28952772e-02, 2.05338809e-02, 1.43737166e-02,\n",
       "         7.07392197e-01, 3.49075975e-02],\n",
       "        [2.18037661e-02, 1.18929633e-02, 0.00000000e+00, 9.91080278e-03,\n",
       "         1.38751239e-02, 7.92864222e-03, 2.97324083e-03, 2.97324083e-02,\n",
       "         0.00000000e+00, 9.01883053e-01]])}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.LadderNetwork import LadderNetwork\n",
    "from LAMDA_SSL.Opitimizer.Adam import Adam\n",
    "from LAMDA_SSL.Dataloader.UnlabeledDataloader import UnlabeledDataLoader\n",
    "from LAMDA_SSL.Dataloader.LabeledDataloader import LabeledDataLoader\n",
    "from LAMDA_SSL.Sampler.RandomSampler import RandomSampler\n",
    "from LAMDA_SSL.Sampler.SequentialSampler import SequentialSampler\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Top_k_Accuracy import Top_k_Accurary\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.LabeledDataset import LabeledDataset\n",
    "from LAMDA_SSL.Dataset.UnlabeledDataset import UnlabeledDataset\n",
    "from LAMDA_SSL.Scheduler.LinearWarmup import LinearWarmup\n",
    "from LAMDA_SSL.Dataset.Vision.Mnist import Mnist\n",
    "import torch.nn as nn\n",
    "\n",
    "dataset=Mnist(root='Download\\mnist',labeled_size=6000,stratified=True,shuffle=True,download=False,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "valid_X=dataset.valid_X\n",
    "valid_y=dataset.valid_y\n",
    "\n",
    "labeled_dataset=LabeledDataset(pre_transform=dataset.pre_transform,transforms=dataset.transforms,\n",
    "                               transform=dataset.transform,target_transform=dataset.target_transform)\n",
    "unlabeled_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.unlabeled_transform)\n",
    "valid_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.valid_transform)\n",
    "test_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.test_transform)\n",
    "\n",
    "#dataloader\n",
    "labeled_dataloader=LabeledDataLoader(batch_size=100,num_workers=0,drop_last=True)\n",
    "unlabeled_dataloader=UnlabeledDataLoader(num_workers=0,drop_last=True)\n",
    "valid_dataloader=UnlabeledDataLoader(batch_size=100,num_workers=0,drop_last=False)\n",
    "test_dataloader=UnlabeledDataLoader(batch_size=100,num_workers=0,drop_last=False)\n",
    "\n",
    "# sampler\n",
    "labeled_sampler=RandomSampler(replacement=True,num_samples=100*540)\n",
    "unlabeled_sampler=RandomSampler(replacement=False)\n",
    "test_sampler=SequentialSampler()\n",
    "valid_sampler=SequentialSampler()\n",
    "\n",
    "# optimizer\n",
    "optimizer=Adam(lr=0.02)\n",
    "\n",
    "# scheduler\n",
    "scheduler=LinearWarmup(num_warmup_steps=15,num_training_steps=10,verbose=False)\n",
    "\n",
    "# evalutation\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'top_5_accuracy':Top_k_Accurary(k=5),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "file = open(\"Result/LadderNetwork_MNIST.txt\", \"w\")\n",
    "\n",
    "model=LadderNetwork(noise_std=0.2,\n",
    "                     lambda_u=[0.1, 0.1, 0.1, 0.1, 0.1, 10., 1000.],\n",
    "                     dim_encoder=[1000, 500, 250, 250, 250],\n",
    "                     encoder_activations=[nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU(), nn.ReLU()],\n",
    "                     mu=1,weight_decay=5e-4,\n",
    "                     epoch=2,num_it_epoch=10,num_it_total=540*10,eval_epoch=1,\n",
    "                     optimizer=optimizer,scheduler=scheduler,evaluation=evaluation,device='cpu',\n",
    "                     labeled_dataset=labeled_dataset, unlabeled_dataset=unlabeled_dataset, valid_dataset=valid_dataset,\n",
    "                     test_dataset=test_dataset,\n",
    "                     labeled_sampler=labeled_sampler, unlabeled_sampler=unlabeled_sampler, valid_sampler=valid_sampler,\n",
    "                     test_sampler=test_sampler,\n",
    "                     labeled_dataloader=labeled_dataloader, unlabeled_dataloader=unlabeled_dataloader,\n",
    "                     valid_dataloader=valid_dataloader, test_dataloader=test_dataloader,\n",
    "                     file=None,verbose=False)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X,valid_X=valid_X,valid_y=valid_y)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc5876",
   "metadata": {},
   "source": [
    "## <font color='green'>$\\Pi$-model</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Perturbation-based -> <font color='green'>$\\Pi$-model</font>\n",
    "\n",
    "### Opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35baced8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to Download\\cifar-10-python\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee2080f810049e6a2cb7b1bdf3533e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Download\\cifar-10-python\\cifar-10-python.tar.gz to Download\\cifar-10-python\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "accuracy   0.1003\n",
      "top_5_accuracy   0.5001\n",
      "precision   0.076775956284153\n",
      "Recall   0.10029999999999999\n",
      "F1   0.0191558241070029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.5031681\n",
      "Confusion_matrix   [[0.    0.    0.    0.    0.    0.    0.075 0.    0.001 0.924]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.021 0.    0.    0.979]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.    1.   ]\n",
      " [0.    0.    0.    0.    0.    0.    0.016 0.    0.004 0.98 ]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.001 0.999]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a5342ce92e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabeled_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabeled_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munlabeled_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munlabeled_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m \u001b[0mperformance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Base\\DeepModelMixin.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, X, y, valid)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Algorithm\\Classification\\PiModel.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, valid)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDeepModelMixin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Base\\DeepModelMixin.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, valid)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_estimate_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_batch_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Base\\DeepModelMixin.py\u001b[0m in \u001b[0;36mpredict_batch_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    391\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m                 \u001b[0m_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m                 \u001b[0m_est\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_est\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m  \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_est\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_est\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_est\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_est\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_est\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Base\\DeepModelMixin.py\u001b[0m in \u001b[0;36mestimate\u001b[1;34m(self, X, idx, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mestimate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Network\\WideResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Network\\WideResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\LAMDA_SSL\\Network\\WideResNet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequalInOut\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from LAMDA_SSL.Augmentation.Vision.RandomHorizontalFlip import RandomHorizontalFlip\n",
    "from LAMDA_SSL.Augmentation.Vision.RandomCrop import RandomCrop\n",
    "from LAMDA_SSL.Dataset.Vision.CIFAR10 import CIFAR10\n",
    "from LAMDA_SSL.Opitimizer.SGD import SGD\n",
    "from LAMDA_SSL.Scheduler.CosineAnnealingLR import CosineAnnealingLR\n",
    "from LAMDA_SSL.Network.WideResNet import WideResNet\n",
    "from LAMDA_SSL.Dataloader.UnlabeledDataloader import UnlabeledDataLoader\n",
    "from LAMDA_SSL.Dataloader.LabeledDataloader import LabeledDataLoader\n",
    "from LAMDA_SSL.Algorithm.Classification.PiModel import PiModel\n",
    "from LAMDA_SSL.Sampler.RandomSampler import RandomSampler\n",
    "from LAMDA_SSL.Sampler.SequentialSampler import SequentialSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Top_k_Accuracy import Top_k_Accurary\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.LabeledDataset import LabeledDataset\n",
    "from LAMDA_SSL.Dataset.UnlabeledDataset import UnlabeledDataset\n",
    "\n",
    "# dataset\n",
    "dataset=CIFAR10(root='Download\\cifar-10-python',labeled_size=4000,stratified=True,shuffle=True,download=False,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "valid_X=dataset.valid_X\n",
    "valid_y=dataset.valid_y\n",
    "\n",
    "labeled_dataset=LabeledDataset(pre_transform=dataset.pre_transform,transforms=dataset.transforms,\n",
    "                               transform=dataset.transform,target_transform=dataset.target_transform)\n",
    "\n",
    "unlabeled_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.unlabeled_transform)\n",
    "\n",
    "valid_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.valid_transform)\n",
    "\n",
    "test_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.test_transform)\n",
    "\n",
    "# sampler\n",
    "labeled_sampler=RandomSampler(replacement=True,num_samples=64*(2**20))\n",
    "unlabeled_sampler=RandomSampler(replacement=True)\n",
    "valid_sampler=SequentialSampler()\n",
    "test_sampler=SequentialSampler()\n",
    "\n",
    "#dataloader\n",
    "labeled_dataloader=LabeledDataLoader(batch_size=64,num_workers=0,drop_last=True)\n",
    "unlabeled_dataloader=UnlabeledDataLoader(num_workers=0,drop_last=True)\n",
    "valid_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "test_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "\n",
    "# augmentation\n",
    "\n",
    "augmentation=Pipeline([('RandomHorizontalFlip',RandomHorizontalFlip()),\n",
    "                        ('RandomCrop',RandomCrop(padding=0.125,padding_mode='reflect')),\n",
    "                      ])\n",
    "\n",
    "# optimizer\n",
    "optimizer=SGD(lr=0.03,momentum=0.9,nesterov=True)\n",
    "\n",
    "# scheduler\n",
    "scheduler=CosineAnnealingLR(eta_min=0,T_max=2**20)\n",
    "\n",
    "# network\n",
    "network=WideResNet(num_classes=10,depth=28,widen_factor=2,drop_rate=0)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'top_5_accuracy':Top_k_Accurary(k=5),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "file = open(\"Result/PiModel_CIFAR10.txt\", \"w\")\n",
    "\n",
    "model=PiModel(lambda_u=10,warmup=0.4,mu=1,weight_decay=5e-4,ema_decay=0.999,\n",
    "               epoch=1,num_it_epoch=10,num_it_total=540*10,\n",
    "               eval_it=2000,device='cpu',\n",
    "               labeled_dataset=labeled_dataset,\n",
    "               unlabeled_dataset=unlabeled_dataset,\n",
    "               valid_dataset=valid_dataset,\n",
    "               test_dataset=test_dataset,\n",
    "               labeled_sampler=labeled_sampler,\n",
    "               unlabeled_sampler=unlabeled_sampler,\n",
    "               valid_sampler=valid_sampler,\n",
    "               test_sampler=test_sampler,\n",
    "               labeled_dataloader=labeled_dataloader,\n",
    "               unlabeled_dataloader=unlabeled_dataloader,\n",
    "               valid_dataloader=valid_dataloader,\n",
    "               test_dataloader=test_dataloader,\n",
    "               augmentation=augmentation,\n",
    "               network=network,\n",
    "               optimizer=optimizer,\n",
    "               scheduler=scheduler,\n",
    "               evaluation=evaluation,\n",
    "               file=file,verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X,valid_X=valid_X,valid_y=valid_y)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb4bb8",
   "metadata": {},
   "source": [
    "## <font color='green'>Temporal ensembling</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Perturbation-based -> <font color='green'>Temporal ensembling</font>\n",
    "\n",
    "### Opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4759583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "accuracy   0.098\n",
      "top_5_accuracy   0.4996\n",
      "precision   0.022668997668997666\n",
      "Recall   0.098\n",
      "F1   0.018852372164484196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.4748847111111112\n",
      "Confusion_matrix   [[0.    0.018 0.    0.    0.    0.    0.032 0.    0.948 0.002]\n",
      " [0.    0.005 0.    0.    0.    0.    0.    0.    0.995 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.007 0.    0.991 0.001]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.003 0.    0.    0.    0.    0.001 0.    0.996 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.002 0.    0.    0.    0.    0.    0.    0.998 0.   ]\n",
      " [0.    0.007 0.    0.    0.    0.    0.017 0.    0.975 0.001]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.    0.   ]]\n",
      "accuracy   0.098\n",
      "top_5_accuracy   0.4996\n",
      "precision   0.022668997668997666\n",
      "Recall   0.098\n",
      "F1   0.018852372164484196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.4748847111111112\n",
      "Confusion_matrix   [[0.    0.018 0.    0.    0.    0.    0.032 0.    0.948 0.002]\n",
      " [0.    0.005 0.    0.    0.    0.    0.    0.    0.995 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.007 0.    0.991 0.001]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.003 0.    0.    0.    0.    0.001 0.    0.996 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.001 0.    0.    0.    0.    0.    0.    0.999 0.   ]\n",
      " [0.    0.002 0.    0.    0.    0.    0.    0.    0.998 0.   ]\n",
      " [0.    0.007 0.    0.    0.    0.    0.017 0.    0.975 0.001]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    1.    0.   ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.098,\n",
       " 'top_5_accuracy': 0.4996,\n",
       " 'precision': 0.022668997668997666,\n",
       " 'Recall': 0.098,\n",
       " 'F1': 0.018852372164484196,\n",
       " 'AUC': 0.4748847111111112,\n",
       " 'Confusion_matrix': array([[0.   , 0.018, 0.   , 0.   , 0.   , 0.   , 0.032, 0.   , 0.948,\n",
       "         0.002],\n",
       "        [0.   , 0.005, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.995,\n",
       "         0.   ],\n",
       "        [0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.007, 0.   , 0.991,\n",
       "         0.001],\n",
       "        [0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999,\n",
       "         0.   ],\n",
       "        [0.   , 0.003, 0.   , 0.   , 0.   , 0.   , 0.001, 0.   , 0.996,\n",
       "         0.   ],\n",
       "        [0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999,\n",
       "         0.   ],\n",
       "        [0.   , 0.001, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.999,\n",
       "         0.   ],\n",
       "        [0.   , 0.002, 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.998,\n",
       "         0.   ],\n",
       "        [0.   , 0.007, 0.   , 0.   , 0.   , 0.   , 0.017, 0.   , 0.975,\n",
       "         0.001],\n",
       "        [0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 0.   , 1.   ,\n",
       "         0.   ]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Augmentation.Vision.RandomHorizontalFlip import RandomHorizontalFlip\n",
    "from LAMDA_SSL.Augmentation.Vision.RandomCrop import RandomCrop\n",
    "from LAMDA_SSL.Dataset.Vision.CIFAR10 import CIFAR10\n",
    "from LAMDA_SSL.Opitimizer.SGD import SGD\n",
    "from LAMDA_SSL.Scheduler.CosineAnnealingLR import CosineAnnealingLR\n",
    "from LAMDA_SSL.Network.WideResNet import WideResNet\n",
    "from LAMDA_SSL.Dataloader.UnlabeledDataloader import UnlabeledDataLoader\n",
    "from LAMDA_SSL.Dataloader.LabeledDataloader import LabeledDataLoader\n",
    "from LAMDA_SSL.Algorithm.Classification.TemporalEnsembling import TemporalEnsembling\n",
    "from LAMDA_SSL.Sampler.RandomSampler import RandomSampler\n",
    "from LAMDA_SSL.Sampler.SequentialSampler import SequentialSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Top_k_Accuracy import Top_k_Accurary\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.LabeledDataset import LabeledDataset\n",
    "from LAMDA_SSL.Dataset.UnlabeledDataset import UnlabeledDataset\n",
    "\n",
    "# dataset\n",
    "dataset=CIFAR10(root='Download\\cifar-10-python',labeled_size=4000,stratified=True,shuffle=True,download=False,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "valid_X=dataset.valid_X\n",
    "valid_y=dataset.valid_y\n",
    "\n",
    "labeled_dataset=LabeledDataset(pre_transform=dataset.pre_transform,transforms=dataset.transforms,\n",
    "                               transform=dataset.transform,target_transform=dataset.target_transform)\n",
    "\n",
    "unlabeled_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.unlabeled_transform)\n",
    "\n",
    "valid_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.valid_transform)\n",
    "\n",
    "test_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.test_transform)\n",
    "\n",
    "# sampler\n",
    "labeled_sampler=RandomSampler(replacement=True,num_samples=460*100)\n",
    "unlabeled_sampler=RandomSampler(replacement=False)\n",
    "valid_sampler=SequentialSampler()\n",
    "test_sampler=SequentialSampler()\n",
    "\n",
    "#dataloader\n",
    "labeled_dataloader=LabeledDataLoader(batch_size=100,num_workers=0,drop_last=True)\n",
    "unlabeled_dataloader=UnlabeledDataLoader(num_workers=0,drop_last=True)\n",
    "valid_dataloader=UnlabeledDataLoader(batch_size=100,num_workers=0,drop_last=False)\n",
    "test_dataloader=UnlabeledDataLoader(batch_size=100,num_workers=0,drop_last=False)\n",
    "\n",
    "# augmentation\n",
    "augmentation=Pipeline([('RandomHorizontalFlip',RandomHorizontalFlip()),\n",
    "                        ('RandomCrop',RandomCrop(padding=0.125,padding_mode='reflect')),\n",
    "                    ])\n",
    "\n",
    "# optimizer\n",
    "optimizer=SGD(lr=0.1,momentum=0.9,weight_decay=5e-4)\n",
    "\n",
    "# scheduler\n",
    "scheduler=CosineAnnealingLR(eta_min=0.0001,T_max=400)\n",
    "\n",
    "# network\n",
    "network=WideResNet(num_classes=10,depth=28,widen_factor=2,drop_rate=0)\n",
    "\n",
    "# evalutation\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'top_5_accuracy':Top_k_Accurary(k=5),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "file = open(\"Result/TemporalEnsembling_CIFAR10.txt\", \"w\")\n",
    "\n",
    "model=TemporalEnsembling(lambda_u=30,ema_weight=0.6,warmup=0.4,mu=1,weight_decay=5e-4,epoch=1,num_it_epoch=10,\n",
    "                         num_it_total=540*10,eval_epoch=10,device='cpu',\n",
    "                         labeled_dataset=labeled_dataset,\n",
    "                         unlabeled_dataset=unlabeled_dataset,\n",
    "                         valid_dataset=valid_dataset,\n",
    "                         test_dataset=test_dataset,\n",
    "                         labeled_sampler=labeled_sampler,\n",
    "                         unlabeled_sampler=unlabeled_sampler,\n",
    "                         valid_sampler=valid_sampler,\n",
    "                         test_sampler=test_sampler,\n",
    "                         labeled_dataloader=labeled_dataloader,\n",
    "                         unlabeled_dataloader=unlabeled_dataloader,\n",
    "                         valid_dataloader=valid_dataloader,\n",
    "                         test_dataloader=test_dataloader,\n",
    "                         augmentation=augmentation,\n",
    "                         network=network,\n",
    "                         optimizer=optimizer,\n",
    "                         scheduler=scheduler,\n",
    "                         evaluation=evaluation,\n",
    "                         file=file, verbose=True\n",
    "                         )\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X,valid_X=valid_X,valid_y=valid_y)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426b0fb",
   "metadata": {},
   "source": [
    "## <font color='green'>Mean teacher</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Perturbation-based -> <font color='green'>Mean teacher</font>\n",
    "\n",
    "### Opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "149cf9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "accuracy   0.1\n",
      "top_5_accuracy   0.4983\n",
      "precision   0.01\n",
      "Recall   0.1\n",
      "F1   0.01818181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.5442325333333334\n",
      "Confusion_matrix   [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "accuracy   0.1\n",
      "top_5_accuracy   0.4983\n",
      "precision   0.01\n",
      "Recall   0.1\n",
      "F1   0.01818181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.5442325333333334\n",
      "Confusion_matrix   [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.1,\n",
       " 'top_5_accuracy': 0.4983,\n",
       " 'precision': 0.01,\n",
       " 'Recall': 0.1,\n",
       " 'F1': 0.01818181818181818,\n",
       " 'AUC': 0.5442325333333334,\n",
       " 'Confusion_matrix': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Augmentation.Vision.RandomHorizontalFlip import RandomHorizontalFlip\n",
    "from LAMDA_SSL.Augmentation.Vision.RandomCrop import RandomCrop\n",
    "from LAMDA_SSL.Dataset.Vision.CIFAR10 import CIFAR10\n",
    "from LAMDA_SSL.Opitimizer.SGD import SGD\n",
    "from LAMDA_SSL.Scheduler.CosineAnnealingLR import CosineAnnealingLR\n",
    "from LAMDA_SSL.Network.WideResNet import WideResNet\n",
    "from LAMDA_SSL.Dataloader.UnlabeledDataloader import UnlabeledDataLoader\n",
    "from LAMDA_SSL.Dataloader.LabeledDataloader import LabeledDataLoader\n",
    "from LAMDA_SSL.Algorithm.Classification.MeanTeacher import MeanTeacher\n",
    "from LAMDA_SSL.Sampler.RandomSampler import RandomSampler\n",
    "from LAMDA_SSL.Sampler.SequentialSampler import SequentialSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Top_k_Accuracy import Top_k_Accurary\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.LabeledDataset import LabeledDataset\n",
    "from LAMDA_SSL.Dataset.UnlabeledDataset import UnlabeledDataset\n",
    "\n",
    "# dataset\n",
    "dataset=CIFAR10(root='Download\\cifar-10-python',labeled_size=4000,stratified=True,shuffle=True,download=False,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "valid_X=dataset.valid_X\n",
    "valid_y=dataset.valid_y\n",
    "\n",
    "labeled_dataset=LabeledDataset(pre_transform=dataset.pre_transform,transforms=dataset.transforms,\n",
    "                               transform=dataset.transform,target_transform=dataset.target_transform)\n",
    "\n",
    "unlabeled_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.unlabeled_transform)\n",
    "\n",
    "valid_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.valid_transform)\n",
    "\n",
    "test_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.test_transform)\n",
    "\n",
    "# sampler\n",
    "labeled_sampler=RandomSampler(replacement=True,num_samples=64*(2**20))\n",
    "unlabeled_sampler=RandomSampler(replacement=True)\n",
    "valid_sampler=SequentialSampler()\n",
    "test_sampler=SequentialSampler()\n",
    "\n",
    "#dataloader\n",
    "labeled_dataloader=LabeledDataLoader(batch_size=64,num_workers=0,drop_last=True)\n",
    "unlabeled_dataloader=UnlabeledDataLoader(num_workers=0,drop_last=True)\n",
    "valid_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "test_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "\n",
    "# augmentation\n",
    "augmentation=Pipeline([('RandomHorizontalFlip',RandomHorizontalFlip()),\n",
    "                        ('RandomCrop',RandomCrop(padding=0.125,padding_mode='reflect')),\n",
    "                      ])\n",
    "\n",
    "# optimizer\n",
    "optimizer=SGD(lr=0.03,momentum=0.9,nesterov=True)\n",
    "\n",
    "# scheduler\n",
    "scheduler=CosineAnnealingLR(eta_min=0,T_max=2**20)\n",
    "\n",
    "# network\n",
    "network=WideResNet(num_classes=10,depth=28,widen_factor=2,drop_rate=0)\n",
    "\n",
    "# evalutation\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'top_5_accuracy':Top_k_Accurary(k=5),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "file = open(\"Result/MeanTeacher_CIFAR10.txt\", \"w\")\n",
    "\n",
    "model=MeanTeacher(lambda_u=50,warmup=0.4,mu=1,weight_decay=5e-4,ema_decay=0.999,\n",
    "                  epoch=1, num_it_epoch=10, num_it_total=540*10, eval_it=2000, device='cpu',\n",
    "                  labeled_dataset=labeled_dataset,\n",
    "                  unlabeled_dataset=unlabeled_dataset,\n",
    "                  valid_dataset=valid_dataset,\n",
    "                  test_dataset=test_dataset,\n",
    "                  labeled_sampler=labeled_sampler,\n",
    "                  unlabeled_sampler=unlabeled_sampler,\n",
    "                  valid_sampler=valid_sampler,\n",
    "                  test_sampler=test_sampler,\n",
    "                  labeled_dataloader=labeled_dataloader,\n",
    "                  unlabeled_dataloader=unlabeled_dataloader,\n",
    "                  valid_dataloader=valid_dataloader,\n",
    "                  test_dataloader=test_dataloader,\n",
    "                  augmentation=augmentation,\n",
    "                  network=network,\n",
    "                  optimizer=optimizer,\n",
    "                  scheduler=scheduler,\n",
    "                  evaluation=evaluation,\n",
    "                  file=file,\n",
    "                  verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X,valid_X=valid_X,valid_y=valid_y)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26dab36",
   "metadata": {},
   "source": [
    "## <font color='green'>VAT Virtual Adversarial Training</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Intrinsically semi-supervised -> Perturbation-based -> <font color='green'>Virtual Adversarial Training</font>\n",
    "\n",
    "### Opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b306cc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "accuracy   0.1\n",
      "top_5_accuracy   0.4656\n",
      "precision   0.01\n",
      "Recall   0.1\n",
      "F1   0.01818181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.4791982777777777\n",
      "Confusion_matrix   [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "accuracy   0.1\n",
      "top_5_accuracy   0.4656\n",
      "precision   0.01\n",
      "Recall   0.1\n",
      "F1   0.01818181818181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kaami\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC   0.4791982777777777\n",
      "Confusion_matrix   [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.1,\n",
       " 'top_5_accuracy': 0.4656,\n",
       " 'precision': 0.01,\n",
       " 'Recall': 0.1,\n",
       " 'F1': 0.01818181818181818,\n",
       " 'AUC': 0.4791982777777777,\n",
       " 'Confusion_matrix': array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Augmentation.Vision.RandomHorizontalFlip import RandomHorizontalFlip\n",
    "from LAMDA_SSL.Augmentation.Vision.RandomCrop import RandomCrop\n",
    "from LAMDA_SSL.Dataset.Vision.CIFAR10 import CIFAR10\n",
    "from LAMDA_SSL.Opitimizer.SGD import SGD\n",
    "from LAMDA_SSL.Scheduler.CosineAnnealingLR import CosineAnnealingLR\n",
    "from LAMDA_SSL.Network.WideResNet import WideResNet\n",
    "from LAMDA_SSL.Dataloader.UnlabeledDataloader import UnlabeledDataLoader\n",
    "from LAMDA_SSL.Dataloader.LabeledDataloader import LabeledDataLoader\n",
    "from LAMDA_SSL.Algorithm.Classification.VAT import VAT\n",
    "from LAMDA_SSL.Sampler.RandomSampler import RandomSampler\n",
    "from LAMDA_SSL.Sampler.SequentialSampler import SequentialSampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Evaluation.Classifier.Top_k_Accuracy import Top_k_Accurary\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "from LAMDA_SSL.Dataset.LabeledDataset import LabeledDataset\n",
    "from LAMDA_SSL.Dataset.UnlabeledDataset import UnlabeledDataset\n",
    "\n",
    "# dataset\n",
    "dataset=CIFAR10(root='Download\\cifar-10-python',labeled_size=4000,stratified=True,shuffle=True,download=False,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "valid_X=dataset.valid_X\n",
    "valid_y=dataset.valid_y\n",
    "\n",
    "labeled_dataset=LabeledDataset(pre_transform=dataset.pre_transform,transforms=dataset.transforms,\n",
    "                               transform=dataset.transform,target_transform=dataset.target_transform)\n",
    "\n",
    "unlabeled_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.unlabeled_transform)\n",
    "\n",
    "valid_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.valid_transform)\n",
    "\n",
    "test_dataset=UnlabeledDataset(pre_transform=dataset.pre_transform,transform=dataset.test_transform)\n",
    "\n",
    "# sampler\n",
    "labeled_sampler=RandomSampler(replacement=True,num_samples=64*(2**20))\n",
    "unlabeled_sampler=RandomSampler(replacement=True)\n",
    "valid_sampler=SequentialSampler()\n",
    "test_sampler=SequentialSampler()\n",
    "\n",
    "#dataloader\n",
    "labeled_dataloader=LabeledDataLoader(batch_size=64,num_workers=0,drop_last=True)\n",
    "unlabeled_dataloader=UnlabeledDataLoader(num_workers=0,drop_last=True)\n",
    "valid_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "test_dataloader=UnlabeledDataLoader(batch_size=64,num_workers=0,drop_last=False)\n",
    "\n",
    "# augmentation\n",
    "\n",
    "augmentation=Pipeline([('RandomHorizontalFlip',RandomHorizontalFlip()),\n",
    "                        ('RandomCrop',RandomCrop(padding=0.125,padding_mode='reflect')),\n",
    "                      ])\n",
    "\n",
    "# optimizer\n",
    "optimizer=SGD(lr=0.03,momentum=0.9,nesterov=True)\n",
    "\n",
    "# scheduler\n",
    "scheduler=CosineAnnealingLR(eta_min=0,T_max=2**20)\n",
    "\n",
    "# network\n",
    "network=WideResNet(num_classes=10,depth=28,widen_factor=2,drop_rate=0)\n",
    "\n",
    "# evalutation\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'top_5_accuracy':Top_k_Accurary(k=5),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "file = open(\"Result/VAT_CIFAR10.txt\", \"w\")\n",
    "\n",
    "model=VAT(lambda_u=0.3,lambda_entmin=0.06,eps=6,xi=1e-6,it_vat=1,warmup=0.4,mu=1,\n",
    "          weight_decay=5e-4, ema_decay=0.999,\n",
    "          epoch=1, num_it_epoch=10, num_it_total=540*10,\n",
    "          eval_it=2000, device='cpu',\n",
    "          labeled_dataset=labeled_dataset,\n",
    "          unlabeled_dataset=unlabeled_dataset,\n",
    "          valid_dataset=valid_dataset,\n",
    "          test_dataset=test_dataset,\n",
    "          labeled_sampler=labeled_sampler,\n",
    "          unlabeled_sampler=unlabeled_sampler,\n",
    "          valid_sampler=valid_sampler,\n",
    "          test_sampler=test_sampler,\n",
    "          labeled_dataloader=labeled_dataloader,\n",
    "          unlabeled_dataloader=unlabeled_dataloader,\n",
    "          valid_dataloader=valid_dataloader,\n",
    "          test_dataloader=test_dataloader,\n",
    "          augmentation=augmentation,\n",
    "          network=network,\n",
    "          optimizer=optimizer,\n",
    "          scheduler=scheduler,\n",
    "          evaluation=evaluation,\n",
    "          file=file, verbose=True\n",
    "          )\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X,valid_X=valid_X,valid_y=valid_y)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e642bc8",
   "metadata": {},
   "source": [
    "# Transductive\n",
    "\n",
    "## <font color='green'>TSVM</font> LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Transductive -> <font color='green'>TSVM</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[LAMDA-SSl]\n",
    "\n",
    "1. Rozpoczęcie od tradycyjnego SVM: TSVM zaczyna od wytrenowania standardowego SVM wykorzystującego oznaczone próbki danych. Hiperpłaszczyzna uzyskana z tego kroku jest używana jako punkt wyjścia do dalszej optymalizacji.\n",
    "\n",
    "2. Próba etykietowania próbek nieoznaczonych: Używając wytrenowanego SVM, TSVM przewiduje etykiety dla próbek nieoznaczonych. Należy jednak pamiętać, że te etykiety są początkowymi, przybliżonymi przewidywaniami i mogą się zmieniać w kolejnych krokach.\n",
    "\n",
    "3. Optymalizacja hiperpłaszczyzny: TSVM dąży do optymalizacji hiperpłaszczyzny dzielącej tak, aby zminimalizować błędy klasyfikacji zarówno dla próbek oznaczonych, jak i dla tych początkowo etykietowanych próbek nieoznaczonych. Proces ten może być realizowany przez iteracyjną procedurę, gdzie w każdej iteracji TSVM aktualizuje etykiety dla próbek nieoznaczonych oraz dostosowuje hiperpłaszczyznę dzielącą.\n",
    "\n",
    "4. Kary i ważność próbek: Ważność przywiązywana do błędów w klasyfikacji różni się dla próbek oznaczonych i nieoznaczonych. Dlatego TSVM wprowadza różne kary (lub wagi) dla tych dwóch rodzajów próbek. W trakcie iteracji, ważność próbek nieoznaczonych jest dostosowywana, co może prowadzić do zmiany ich etykietowania.\n",
    "\n",
    "5. Zakończenie procesu: Proces iteracyjny jest kontynuowany do momentu, gdy spełnione są pewne kryteria zbieżności (na przykład gdy liczba błędnie etykietowanych próbek nieoznaczonych nie zmienia się między kolejnymi iteracjami lub osiągnie się określony próg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35055cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9590643274853801,\n",
       " 'precision': 0.9647397094430993,\n",
       " 'Recall': 0.9484521028037383,\n",
       " 'F1': 0.9555629802873371,\n",
       " 'AUC': 0.9941588785046729,\n",
       " 'Confusion_matrix': array([[0.90625   , 0.09375   ],\n",
       "        [0.00934579, 0.99065421]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.TSVM import TSVM\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/TSVM_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "model=TSVM(evaluation=evaluation,file=file)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y,Transductive=False)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492a157",
   "metadata": {},
   "source": [
    "# <font color='blue'>Graph-based</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be78e4",
   "metadata": {},
   "source": [
    "## <font color='green'>Label propagation algorithm for inference on graphs</font> <font color='orange'> SKLEARN</font> or LAMDA-SSL\n",
    "\n",
    "###  Semi-supervised classification -> Transductive -> Graph-based -> Transductive -> Inference in graphs -> <font color='green'>Label propagation</font>\n",
    "\n",
    "### Opis\n",
    "\n",
    "[LAMDA-SSL]\n",
    "\n",
    "Label Propagation to metoda uczenia półnadzorowanego zaproponowana przez Zhu i innych. Wykorzystuje ona próbki jako węzły oraz ich relacje jako krawędzie w grafie. Graf ten może być w pełni połączony lub oparty na k-najbliższych sąsiadach. Głównym celem algorytmu Label Propagation jest przekazywanie etykiet z oznaczonych danych do nieoznaczonych przez graf. Optymalizacja polega na zapewnieniu spójności Laplace'a, czyli minimalizacji ważonego błędu średniokwadratowego różnic etykiet między sąsiadującymi węzłami. Ponieważ etykiety próbek oznaczonych są stałe, algorytm skupia się na etykietowaniu danych nieoznaczonych. Metoda ta oferuje optymalne rozwiązanie w postaci zamkniętej, które jest zbieżne z wynikiem nieskończonych iteracji. Jedną z głównych zalet Label Propagation w porównaniu z innymi metodami półnadzorowanymi opartymi na grafie jest jej zdolność do uzyskiwania dokładnych rozwiązań bez potrzeby wielokrotnej symulacji procesu propagacji etykiet.\n",
    "\n",
    "[A survey on semi-supervised learning]\n",
    "\n",
    "Algorytm Label Propagation, zaproponowany przez Zhu i Ghahramani, służy do wnioskowania na grafach. W tym podejściu etykiety są przekazywane (propagowane) z węzła do węzła na podstawie wag krawędzi. Każda estymowana etykieta w węźle jest obliczana jako ważona suma etykiet sąsiednich węzłów. W notacji macierzowej, istnieje macierz przejścia opisująca te ważenia. Algorytm składa się z dwóch kroków: propagacji etykiet do sąsiadujących węzłów i resetowania przewidywań dla oznaczonych punktów danych do ich prawdziwych etykiet. Procedura jest powtarzana do momentu zbieżności.\n",
    "\n",
    "Zhu w 2005 roku udowodnił, że algorytm zbiega do funkcji harmonicznego rozwiązania. Pokazał również, że Label Propagation może być interpretowany jako losowy spacer z macierzą przejścia, który zatrzymuje się, gdy osiągnie węzeł z etykietą. Ten sposób patrzenia jest zbliżony do podejścia opartego na Markovskich losowych spacerach, gdzie istnieje prawdopodobieństwo zatrzymania spaceru, gdy osiągnie węzeł z etykietą. Istnieją również prace innych badaczy, które przedstawiają podobne podejścia lub rozszerzenia algorytmu Label Propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9bfd847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9532163742690059,\n",
       " 'precision': 0.9652173913043478,\n",
       " 'Recall': 0.9375,\n",
       " 'F1': 0.9486486486486486,\n",
       " 'AUC': 0.9976635514018691,\n",
       " 'Confusion_matrix': array([[0.875, 0.125],\n",
       "        [0.   , 1.   ]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from LAMDA_SSL.Algorithm.Classification.LabelPropagation import LabelPropagation\n",
    "from LAMDA_SSL.Evaluation.Classifier.Accuracy import Accuracy\n",
    "from LAMDA_SSL.Dataset.Tabular.BreastCancer import BreastCancer\n",
    "from LAMDA_SSL.Evaluation.Classifier.Recall import Recall\n",
    "from LAMDA_SSL.Evaluation.Classifier.F1 import F1\n",
    "from LAMDA_SSL.Evaluation.Classifier.Precision import Precision\n",
    "from LAMDA_SSL.Evaluation.Classifier.AUC import AUC\n",
    "from LAMDA_SSL.Evaluation.Classifier.Confusion_Matrix import Confusion_Matrix\n",
    "import numpy as np\n",
    "\n",
    "file = open(\"Result/LabelPropagation_BreastCancer.txt\", \"w\")\n",
    "\n",
    "dataset=BreastCancer(test_size=0.3,labeled_size=0.1,stratified=True,shuffle=True,random_state=0,default_transforms=True)\n",
    "\n",
    "labeled_X=dataset.labeled_X\n",
    "labeled_y=dataset.labeled_y\n",
    "unlabeled_X=dataset.unlabeled_X\n",
    "unlabeled_y=dataset.unlabeled_y\n",
    "test_X=dataset.test_X\n",
    "test_y=dataset.test_y\n",
    "\n",
    "# Pre_transform\n",
    "pre_transform=dataset.pre_transform\n",
    "pre_transform.fit(np.vstack([labeled_X, unlabeled_X]))\n",
    "\n",
    "labeled_X=pre_transform.transform(labeled_X)\n",
    "unlabeled_X=pre_transform.transform(unlabeled_X)\n",
    "test_X=pre_transform.transform(test_X)\n",
    "\n",
    "evaluation={\n",
    "    'accuracy':Accuracy(),\n",
    "    'precision':Precision(average='macro'),\n",
    "    'Recall':Recall(average='macro'),\n",
    "    'F1':F1(average='macro'),\n",
    "    'AUC':AUC(multi_class='ovo'),\n",
    "    'Confusion_matrix':Confusion_Matrix(normalize='true')\n",
    "}\n",
    "\n",
    "model=LabelPropagation(gamma=1,max_iter=10000,evaluation=evaluation,file=file,verbose=True)\n",
    "\n",
    "model.fit(X=labeled_X,y=labeled_y,unlabeled_X=unlabeled_X)\n",
    "\n",
    "performance=model.evaluate(X=test_X,y=test_y,Transductive=False)\n",
    "\n",
    "result=model.y_pred\n",
    "\n",
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a8a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSc_venv",
   "language": "python",
   "name": "bsc_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
